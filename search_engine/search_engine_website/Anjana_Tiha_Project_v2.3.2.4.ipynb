{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n* Author           : Anjana Tiha\\n* Course           : Inform Retrieval/Web Search\\n* Course Instructor: Professor Visali Rus\\n* Semester         : Fall 2017\\n* University       : University of Memphis \\n*\\n* Project Name     : Web Search Engine Implementation using Python(Implemented web crawler, preprocessor, inverted document\\n*                    indexer and cosine similarity measurer)  \\n* Last Update      : 12.17.2017\\n*\\n* Description      : 1. Implemented web search engine with query processing and web crawler with web document processor.\\n*                  : 2. Collected 10, 000 documents from \"www.memphis.edu\" web domain including webpages, .pdf, docx, pptx\\n*                       and .txt files.\\n*                  : 3. Implemented query processing engine by genereting TF-IDF for query and calculating cosine simmilarity\\n*                  :    between query and document corpus TF-IDF vector space.\\n*                  : 4. For document preprocessing of webpages/web documents including text files and pdf files:\\n*                       1. Removed HTML and script tags.\\n*                       2. Removing urls.\\n*                       3. Removing special characters and digits.\\n*                       4. Changing upper to lower case letters.\\n*                       5. Remove stop words.\\n*                       6. Stemming all words to root. \\n*                    5. Built inverted document frequency index.\\n*                    6. Generated TF-IDF vector for full document corpus. \\n*                    7. Calculates cosine similarity between query and  documents.\\n*                    8. Returns pages with maximum cosine similarity in decending order.\\n*                    9. Crawler can update and add new documents besides primary crawling.\\n*\\n* Tools Requirement: pdf2text (For .pdf processing)\\n*\\n* Comments         : Please use Anaconda editor for convenience.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "* Author           : Anjana Tiha\n",
    "* Course           : Inform Retrieval/Web Search\n",
    "* Course Instructor: Professor Visali Rus\n",
    "* Semester         : Fall 2017\n",
    "* University       : University of Memphis \n",
    "*\n",
    "* Project Name     : Web Search Engine Implementation using Python(Implemented web crawler, preprocessor, inverted document\n",
    "*                    indexer and cosine similarity measurer)  \n",
    "* Last Update      : 12.17.2017\n",
    "*\n",
    "* Description      : 1. Implemented web search engine with query processing and web crawler with web document processor.\n",
    "*                  : 2. Collected 10, 000 documents from \"www.memphis.edu\" web domain including webpages, .pdf, docx, pptx\n",
    "*                       and .txt files.\n",
    "*                  : 3. Implemented query processing engine by genereting TF-IDF for query and calculating cosine simmilarity\n",
    "*                  :    between query and document corpus TF-IDF vector space.\n",
    "*                  : 4. For document preprocessing of webpages/web documents including text files and pdf files:\n",
    "*                       1. Removed HTML and script tags.\n",
    "*                       2. Removing urls.\n",
    "*                       3. Removing special characters and digits.\n",
    "*                       4. Changing upper to lower case letters.\n",
    "*                       5. Remove stop words.\n",
    "*                       6. Stemming all words to root. \n",
    "*                    5. Built inverted document frequency index.\n",
    "*                    6. Generated TF-IDF vector for full document corpus. \n",
    "*                    7. Calculates cosine similarity between query and  documents.\n",
    "*                    8. Returns pages with maximum cosine similarity in decending order.\n",
    "*                    9. Crawler can update and add new documents besides primary crawling.\n",
    "*\n",
    "* Tools Requirement: pdf2text (For .pdf processing)\n",
    "*\n",
    "* Comments         : Please use Anaconda editor for convenience.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Import modules\n",
    "\n",
    "import os, errno\n",
    "import math\n",
    "import time\n",
    "import operator \n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from collections import deque\n",
    "import queue\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import urllib\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import datetime\n",
    "import numpy as np\n",
    "import docx2txt\n",
    "from pptx import Presentation\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# global Hashmaps, variables\n",
    "stopwords = {}\n",
    "term_doc_freq_vector = {}\n",
    "doc_term_freq_vector = {}\n",
    "page_doc_map = {}\n",
    "doc_page_map = {}\n",
    "page_ref_count = {}\n",
    "doc_count = 0\n",
    "link_queue = queue.Queue()\n",
    "last_doc_index = -1\n",
    "page_queued_map = {}\n",
    "start_time = time.time()\n",
    "total_number_docs = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global directory or file name\n",
    "\n",
    "crawled_web_dir = \"web_text_crawled\"\n",
    "crawled_web_dir_conv_need = \"web_docs_crawled\"\n",
    "crawled_web_dir_preprocessed = \"web_text_preprocessed\"\n",
    "output_web_dir = \"output\"\n",
    "\n",
    "stopword_path = \"english.stopwords.txt\"\n",
    "\n",
    "list_dir = [crawled_web_dir, crawled_web_dir_conv_need, crawled_web_dir_preprocessed]\n",
    "\n",
    "url = \"http://www.memphis.edu/\"\n",
    "#url = \"http://www.cs.memphis.edu/~vrus/teaching/ir-websearch/\"\n",
    "domain = \"memphis.edu\"\n",
    "total_number_docs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File Operations\n",
    "# Create/Delete/ file/Directory \n",
    "\n",
    "# create one single directory\n",
    "def create_directory(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if(e.errno != errno.EEXIST):\n",
    "            raise\n",
    "    pass \n",
    "\n",
    "\n",
    "# create a list of directories\n",
    "def create_directories(list_dir):\n",
    "    for dir_i in list_dir:\n",
    "        print(dir_i)\n",
    "        create_directory(dir_i) \n",
    "        \n",
    "        \n",
    "# delete one single directory\n",
    "def delete_directory(dir_name):\n",
    "    if(os.path.isdir(dir_name)):\n",
    "        try:\n",
    "            shutil.rmtree(dir_name)\n",
    "        except OSError as e:\n",
    "            if(e.errno != errno.EEXIST):\n",
    "                raise\n",
    "        pass \n",
    "\n",
    "    \n",
    "# delete a list of directories\n",
    "def delete_directories(list_dir):\n",
    "    for dir_i in list_dir:\n",
    "        print(dir_i)\n",
    "        delete_directory(dir_i)\n",
    "\n",
    "        \n",
    "#delete if file is empty\n",
    "def delete_file(path):\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except WindowsError:\n",
    "        print(\"failed deleting: \" + path)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#save text in given path with given file name\n",
    "def save_text(text, dir_path, file_name):\n",
    "    text_file = open(dir_path+\"\\\\\"+file_name, \"w\")\n",
    "    text_file.write(text)\n",
    "    text_file.close()\n",
    "        \n",
    "        \n",
    "#load stop words from given file path\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r') as content_file:\n",
    "        for line in content_file:\n",
    "            line = line.strip()\n",
    "            stopwords[line] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# save object in pickle\n",
    "def save_obj(obj, name, key_or_val, order):\n",
    "    filename = name + \".p\"\n",
    "    \n",
    "    if(key_or_val == \"key\" and order == \"auto\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0))\n",
    "    elif(key_or_val == \"key\" and order == \"reverse\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0), reverse=True)\n",
    "    elif(key_or_val == \"value\" and order == \"auto\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1))\n",
    "    elif(key_or_val == \"value\" and order == \"reverse\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    if (os.path.isfile(filename)):\n",
    "        os.remove(filename)\n",
    "        \n",
    "    pickle.dump( obj, open( filename, \"wb\" ) )\n",
    "\n",
    "\n",
    "#save object     \n",
    "def save_obj_without_sort(obj, name):\n",
    "    pickle.dump( obj, open( name + \".p\", \"wb\" ) )\n",
    "\n",
    "    \n",
    "# save object in pickle\n",
    "def save_obj_no_sort(obj, name):\n",
    "    filename = name + \".p\"\n",
    "    \n",
    "    if (os.path.isfile(filename)):\n",
    "        os.remove(filename)\n",
    "        \n",
    "    pickle.dump( obj, open( filename, \"wb\" ) )\n",
    "    \n",
    "    \n",
    "#save current queue for future resume of crawling\n",
    "def save_obj_no_sort_w(queue1, filename):\n",
    "    link_list = []\n",
    "    \n",
    "    new_queue = queue.Queue()\n",
    "    new_queue.queue = copy.deepcopy(queue1.queue)\n",
    "    if (os.path.isfile(filename)):\n",
    "        os.remove(filename)\n",
    "        \n",
    "    while(new_queue.empty() == False):\n",
    "        link_list.append(new_queue.get())\n",
    "        \n",
    "    filename = filename + \".p\"\n",
    "    \n",
    "    pickle.dump( link_list, open( filename, \"wb\" ) )\n",
    "\n",
    "\n",
    "    \n",
    "#load object from pickle file\n",
    "def load_obj(name):\n",
    "    file = open(name,'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return object_file\n",
    "  \n",
    "    \n",
    "# save object in pickle\n",
    "def load_obj_no_sort(name):\n",
    "    file = open(name,'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return object_file\n",
    " \n",
    "    \n",
    "#load last visit queue for resume of crawling\n",
    "def load_obj_no_sort_w(filename):\n",
    "    global link_queue\n",
    "    link_list =[]\n",
    "    \n",
    "    if (os.path.isfile(filename)):\n",
    "        file = open(filename,'rb')\n",
    "        link_list = pickle.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        for link in link_list:\n",
    "            link_queue.put(link)\n",
    "        return link_queue\n",
    "    \n",
    "    else:\n",
    "        print(\"no file found\")\n",
    "        return\n",
    "\n",
    "#reset all variables\n",
    "def reset_global_variables():\n",
    "    global stopwords\n",
    "    global term_doc_freq_vector\n",
    "    global doc_term_freq_vector\n",
    "    global page_doc_map\n",
    "    global doc_page_map\n",
    "    global page_ref_count\n",
    "    global doc_count\n",
    "    global link_queue\n",
    "    global last_doc_index\n",
    "    global page_queued_map\n",
    "    global start_time\n",
    "\n",
    "    stopwords = {}\n",
    "    term_doc_freq_vector = {}\n",
    "    doc_term_freq_vector = {}\n",
    "    page_doc_map = {}\n",
    "    doc_page_map = {}\n",
    "    page_ref_count = {}\n",
    "    doc_count = 0\n",
    "    link_queue = queue.Queue()\n",
    "    last_doc_index = -1\n",
    "    page_queued_map = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "#delete all files\n",
    "def delete_all_files():\n",
    "    try:\n",
    "        delete_directories(list_dir)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"doc_count.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"doc_term_freq_vector.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"doc_term_freq_vector_norm.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"doc_url_map.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"link_queue.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"page_ref_count.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"term_doc_freq_vector.p\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        delete_file(\"url_doc_map.p\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "# save crawled doc object    \n",
    "def save_all_obj():\n",
    "    global page_doc_map\n",
    "    global doc_page_map\n",
    "    global page_ref_count\n",
    "    global doc_count\n",
    "    global link_queue\n",
    "    \n",
    "    save_obj(page_doc_map, \"url_doc_map\", \"value\", \"auto\")\n",
    "    save_obj(doc_page_map, \"doc_url_map\", \"key\", \"auto\")\n",
    "    save_obj_no_sort(doc_count, \"doc_count\")\n",
    "    save_obj_no_sort_w(link_queue, \"link_queue\")\n",
    "    save_obj(page_ref_count, \"page_ref_count\", \"value\", \"reverse\")\n",
    "    print(\"saved objects\")\n",
    "\n",
    "#save tfidf    \n",
    "def save_all_obj_tfidf(doc_term_freq_vector_norm):\n",
    "    global term_doc_freq_vector\n",
    "    global doc_term_freq_vector\n",
    "    \n",
    "    save_obj_without_sort(term_doc_freq_vector, \"term_doc_freq_vector\")\n",
    "    save_obj_without_sort(doc_term_freq_vector, \"doc_term_freq_vector\")\n",
    "    save_obj_without_sort(doc_term_freq_vector_norm, \"doc_term_freq_vector_norm\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# load crawled doc object  \n",
    "def load_all_obj():\n",
    "    global page_doc_map\n",
    "    global doc_page_map\n",
    "    global page_ref_count\n",
    "    global doc_count\n",
    "    global link_queue\n",
    "    \n",
    "    page_doc_map = load_obj(\"url_doc_map.p\")\n",
    "    doc_page_map = load_obj(\"doc_url_map.p\")\n",
    "    #doc_count = load_obj_no_sort(\"doc_count.p\")\n",
    "    doc_count = max(doc_page_map.keys())\n",
    "    link_queue = load_obj_no_sort_w(\"link_queue.p\")\n",
    "    page_ref_count = load_obj(\"page_ref_count.p\")\n",
    "    print(\"loaded objects\")\n",
    "\n",
    "\n",
    "def load_obj_search():\n",
    "    global total_number_docs\n",
    "    global doc_url_map\n",
    "    global term_doc_freq_vector\n",
    "    global doc_term_freq_vector\n",
    "\n",
    "    \n",
    "    stopword_path = \"english.stopwords.txt\"\n",
    "    doc_url_map_file = \"doc_url_map.p\"\n",
    "    term_doc_freq_file = \"term_doc_freq_vector.p\"\n",
    "    doc_term_freq_file = \"doc_term_freq_vector.p\"\n",
    "    doc_term_freq_file_norm = \"doc_term_freq_vector_norm.p\"\n",
    "    doc_count = \"doc_count.p\"\n",
    "    \n",
    "    total_number_docs = load_obj(doc_count)\n",
    "    doc_url_map = load_obj(doc_url_map_file)\n",
    "    term_doc_freq_vector = load_obj(term_doc_freq_file)\n",
    "    doc_term_freq_vector = load_obj(doc_term_freq_file)\n",
    "    doc_term_freq_vector_norm = load_obj(doc_term_freq_file_norm)\n",
    "    load_stopwords(stopword_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print elapsed time in hh:mm:ss format\n",
    "def format_time(start_time, end_time):\n",
    "    elsapsed_time = end_time - start_time\n",
    "    hr = int(elsapsed_time)//3600\n",
    "    min_ = (int(elsapsed_time) - (hr * 3600))/60\n",
    "    sec = int(elsapsed_time) - hr * 3600 - min_ * 60\n",
    "    print(\"HH:Min:Sec > \" + str(hr) +\" hr \" + str(min_) + \" min \"+ str(sec) + \"sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# URLS and Text preprocessing functions\n",
    "\n",
    "# remove fragment identifier # and repeated loop url for php and asp\n",
    "def remove_url_frag_id(url):\n",
    "    if \".php\" in url:\n",
    "        url = url.split('.php')\n",
    "        if(len(url)>1):\n",
    "            url =url[0] + \".php\"\n",
    "    elif \".aspx\" in url:\n",
    "        url = url.split('.aspx')\n",
    "        if(len(url)>1):\n",
    "            url =url[0] + \".aspx\"   \n",
    "    url = url.split('#')[0]\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "# remove fragment identifier #\n",
    "def remove_url_frag_simple(url):   \n",
    "    url = url.split('#')[0]\n",
    "    return url\n",
    "\n",
    "\n",
    "# removes \"/\" from url\n",
    "def remove_slash_before_or_after(url, type_r):\n",
    "    if(type_r == \"before\"):\n",
    "        if url.startswith(\"/\"):\n",
    "            url = url[1:]\n",
    "        return url\n",
    "    \n",
    "    elif(type_r == \"after\"):   \n",
    "        if url[-1]==\"/\":\n",
    "            url = url.rsplit('/', 1)[0]\n",
    "        return url \n",
    "    \n",
    "    \n",
    "# remove http or https from webpage urls\n",
    "def strip_http_s(url):    \n",
    "    url = url.replace(\"https://\",\"\")\n",
    "    url = url.replace(\"http://\",\"\")\n",
    "    url = url.rstrip('\\/') \n",
    "    url = \"http://\"+ url\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "# check if url is in selected domain name\n",
    "def check_if_in_domain(url, domain):\n",
    "    if(domain in url): \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "# file type extention type exclusion\n",
    "def is_excluded_type(extension):\n",
    "    exclude_list = [\"jpg\", \"jpeg\", \"png\", \"mp3\", \"mp4\", \"xlx\"]\n",
    "    if extension in exclude_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "#check whether URL is valid\n",
    "def check_valid_URL(url):\n",
    "    url_reg = re.compile(\n",
    "        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "        r'localhost|' #localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?' # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    is_valid = url_reg.match(url)\n",
    "    \n",
    "    return is_valid\n",
    "\n",
    "\n",
    "#get extention of link to check the link type(.txt, .pdf, or html) \n",
    "def get_page_extention(url):\n",
    "    weblink_extention = url.rsplit('.', 1)[-1]\n",
    "    return weblink_extention\n",
    "\n",
    "\n",
    "#remove hyperlink from web page text for preprocessing\n",
    "def remove_hyper_link(text):\n",
    "    URLless_string = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
    "    return URLless_string\n",
    "\n",
    "\n",
    "#remove special character in single line\n",
    "def remove_special_char(line):\n",
    "    line = re.sub('[^a-zA-Z]+', ' ', line)\n",
    "    return line\n",
    "\n",
    "\n",
    "# get all the url/links to other pages from current pages        \n",
    "def get_all_links(url, html):\n",
    "    global domain\n",
    "    global link_queue\n",
    "    global page_queued_map\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    \n",
    "    for tag in links:\n",
    "        link = tag.get('href', None)\n",
    "        \n",
    "        if link is not None:\n",
    "            try:\n",
    "                link_extention = get_page_extention(link)\n",
    "                \n",
    "                if(link == \"\" or link == \"#\" or link_extention == \"ppt\" or is_excluded_type(link_extention) == 1):\n",
    "                    a=1\n",
    "\n",
    "                elif(link_extention in [\"pdf\", \"docx\", \"pptx\", \"txt\"]):\n",
    "                    if(check_valid_URL(link)):\n",
    "                        link_original = strip_http_s(link)\n",
    "                        \n",
    "                        if link_original not in page_queued_map:\n",
    "                            page_queued_map[link_original] = 1\n",
    "                            link_queue.put(link)\n",
    "\n",
    "                    else:\n",
    "                        modified_url = remove_url_frag_id(url)\n",
    "                        modified_url = remove_slash_before_or_after(modified_url, \"after\")\n",
    "                        modified_link = remove_slash_before_or_after(link, \"before\")\n",
    "                        modified_link = modified_url + \"/\" + modified_link\n",
    "                        \n",
    "                        if(check_valid_URL(modified_link)):\n",
    "                            link_original = strip_http_s(modified_link)\n",
    "                            \n",
    "                            if link_original not in page_queued_map:\n",
    "                                page_queued_map[link_original] = 1\n",
    "                                link_queue.put(modified_link)\n",
    "\n",
    "                else:\n",
    "                    is_valid = check_valid_URL(link)\n",
    "                    if(is_valid):\n",
    "                        modified_link = remove_url_frag_id(link)\n",
    "                        modified_link = remove_slash_before_or_after(modified_link, \"after\")\n",
    "                        link_original = strip_http_s(modified_link)\n",
    "                        \n",
    "                        if(check_if_in_domain(modified_link, domain) == 1):\n",
    "                            \n",
    "                            if link_original not in page_queued_map:\n",
    "                                page_queued_map[link_original] = 1\n",
    "                                link_queue.put(modified_link)\n",
    "\n",
    "                    else:\n",
    "                        modified_url= remove_url_frag_id(url)\n",
    "                        modified_url = remove_slash_before_or_after(modified_url, \"after\")\n",
    "                        modified_link = remove_slash_before_or_after(url, \"before\")\n",
    "                        \n",
    "                        if(modified_url!=modified_link):\n",
    "                            \n",
    "                            if  modified_url not in modified_link:\n",
    "                                modified_link = modified_url + \"/\" + modified_link \n",
    "                                \n",
    "                                if(check_if_in_domain(modified_link, domain) == 1):\n",
    "                                    if(check_valid_URL(modified_link)):\n",
    "                                        link_original = strip_http_s(modified_link)\n",
    "                                        \n",
    "                                        if link_original not in page_queued_map:\n",
    "                                            page_queued_map[link_original] = 1\n",
    "                                            link_queue.put(modified_link) \n",
    "                            else: \n",
    "                                if(check_if_in_domain(modified_link, domain) == 1):\n",
    "                                    if(check_valid_URL(modified_link)):\n",
    "                                        link_original = strip_http_s(modified_link)\n",
    "                                        \n",
    "                                        if link_original not in page_queued_map:\n",
    "                                            page_queued_map[link_original] = 1\n",
    "                                            link_queue.put(modified_link)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert pdf to text using \"pdf2text\"\n",
    "def pdf_to_text(input_pdf, file_name):\n",
    "    global crawled_web_dir\n",
    "    os.system((\"pdftotext %s %s\") %( input_pdf, crawled_web_dir+\"//\"+file_name))\n",
    "\n",
    "    \n",
    "# convert pptx to text    \n",
    "def pptx_to_text(book_path):\n",
    "    prs = Presentation(book_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    text = text_runs + run.text\n",
    "    return text\n",
    "\n",
    "\n",
    "#import pdf, docx, pptx from url/single web link and convert to text and save in directory\n",
    "def import_convert_preprocess(url, extension):\n",
    "    global doc_count\n",
    "    global crawled_web_dir_preprocessed\n",
    "    global crawled_web_dir\n",
    "    global crawled_web_dir_conv_need\n",
    "    global page_doc_map\n",
    "    url_map_name = url\n",
    "    \n",
    "    if(url_map_name not in page_doc_map):\n",
    "        page_doc_map[url_map_name] = -1\n",
    "        page_ref_count[url_map_name] = 1\n",
    "        \n",
    "        try:\n",
    "            doc_count_temp = doc_count + 1\n",
    "            book_name = \"\"\n",
    "            if extension == \"pdf\":\n",
    "                book_name = str(doc_count_temp) + \".pdf\"\n",
    "            elif extension == \"docx\":\n",
    "                book_name = str(doc_count_temp) + \".docx\"\n",
    "            elif extension == \"pptx\":\n",
    "                book_name = str(doc_count_temp) + \".pptx\"\n",
    "\n",
    "            book_path = crawled_web_dir_conv_need + \"\\\\\" + book_name\n",
    "            \n",
    "            a = requests.get(url, stream=True)\n",
    "            \n",
    "            with open(book_path, 'wb') as book:   \n",
    "                for block in a.iter_content(512):\n",
    "                    if not block:\n",
    "                        break\n",
    "                    book.write(block)\n",
    "                    \n",
    "            book.close()\n",
    "\n",
    "            file_name = str(doc_count_temp)+\".txt\"\n",
    "            file_path = crawled_web_dir+ \"\\\\\" + file_name\n",
    "            is_valid_for_indexing = 555\n",
    "            if extension == \"pdf\":\n",
    "                pdf_to_text(book_path, file_name)\n",
    "                is_valid_for_indexing = preprocess_one_doc_from_pdf(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
    "                \n",
    "            elif extension == \"docx\":\n",
    "                text = docx2txt.process(book_path)\n",
    "                save_text(text, crawled_web_dir, file_name)\n",
    "                is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
    "                \n",
    "            elif extension == \"pptx\":\n",
    "                text = pptx_to_text(book_path)\n",
    "                save_text(text, crawled_web_dir, file_name)\n",
    "                is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
    "        \n",
    "\n",
    "            if(is_valid_for_indexing == 1) :\n",
    "                doc_count = doc_count + 1\n",
    "                page_doc_map[url_map_name] = doc_count\n",
    "                doc_page_map[doc_count] = url_map_name\n",
    "                page_ref_count[url_map_name] = 1\n",
    "            else:\n",
    "                delete_file(book_path)\n",
    "                delete_file(file_path)\n",
    "                page_doc_map[url_map_name] = -2\n",
    "\n",
    "        except IOError:\n",
    "            page_doc_map[url_map_name]= -1\n",
    "    else:\n",
    "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
    "\n",
    "        \n",
    "\n",
    "#remove all html and scripting\n",
    "def remove_extra_space(txt):\n",
    "    # Removes all blank lines\n",
    "    txt = re.sub(r'\\n\\s*\\n', '\\n', txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "# clean html of the tag and markups\n",
    "def clean_html(html_text):\n",
    "    global crawled_web_dir\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    for script in soup(['style', 'script', 'head', 'title', 'meta', '[document]']):\n",
    "        script.extract()\n",
    "    for tag in soup.find_all('a'):\n",
    "        tag.replaceWith('')\n",
    "    for tag in soup.find_all('footer'):\n",
    "        tag.replaceWith('')\n",
    "    \n",
    "    clean_text = soup.get_text()\n",
    "    clean_text = remove_extra_space(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import text from single web page and preprocess\n",
    "def fetch_extract_html_txt(url):\n",
    "    global doc_count\n",
    "    global domain\n",
    "    global crawled_web_dir\n",
    "    global crawled_web_dir_preprocessed\n",
    "    \n",
    "    if(check_if_in_domain(url, domain) == 0):\n",
    "        return \n",
    "    url_map_name = url\n",
    "    \n",
    "    if(url_map_name in page_doc_map):\n",
    "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
    "        \n",
    "    else:\n",
    "        page_doc_map[url_map_name] = -1\n",
    "        page_ref_count[url_map_name] = 1\n",
    "        \n",
    "        try: \n",
    "            html = urllib.request.urlopen(url) \n",
    "            html_text = html.read() \n",
    "            \n",
    "            if(html_text.strip() == \"\"):\n",
    "                return\n",
    "            \n",
    "            clean_text = clean_html(html_text)\n",
    "            clean_text = clean_text.strip()\n",
    "\n",
    "            if clean_text.strip()==\"\":\n",
    "                return\n",
    "            \n",
    "            doc_count = doc_count + 1\n",
    "            page_doc_map[url_map_name] = doc_count\n",
    "            doc_page_map[doc_count] = url_map_name\n",
    "            save_text(clean_text, crawled_web_dir, str(doc_count)+\".txt\")\n",
    "            \n",
    "            file_name = str(doc_count)+\".txt\"\n",
    "            file_path = crawled_web_dir+ \"\\\\\" + file_name\n",
    "            \n",
    "            is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
    "            \n",
    "            if(is_valid_for_indexing == 0) :\n",
    "                delete_file(file_path)\n",
    "                page_doc_map[url_map_name] = -2\n",
    "                doc_count = doc_count - 1\n",
    "                \n",
    "            get_all_links(url, html_text)\n",
    "            \n",
    "        except:\n",
    "            page_doc_map[url_map_name]= -1\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess files in a folder to remove punctuations, digits, special characters, url/web links\n",
    "# removes stop words given in file\n",
    "# convert to origin word/ do stemming\n",
    "\n",
    "\n",
    "# preprocess webpage text, docx, pptx text and save in directory\n",
    "def preprocess_one_doc(input_dir, input_filename, output_dir):\n",
    "    ps = PorterStemmer()\n",
    "    input_file_path = input_dir + \"\\\\\"+ input_filename\n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r') as content_file:\n",
    "            for line in content_file:\n",
    "                if(line in ['\\n', '\\r\\n','\\r']):\n",
    "                    continue\n",
    "                    \n",
    "                line = line.strip()\n",
    "                line = remove_hyper_link(line)\n",
    "                line = remove_special_char(line)\n",
    "                line = line.lower()\n",
    "                line = re.sub(' +',' ',line)\n",
    "                words = line.split(\" \")\n",
    "                \n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    word = remove_special_char(word)\n",
    "                    word = re.sub(' +','',word)\n",
    "                    \n",
    "                    if word not in stopwords and word != \" \" and word != \"\":\n",
    "                        stem_word = ps.stem(word)\n",
    "                        text = text + \" \" + stem_word\n",
    "                        count = count + 1       \n",
    "                        \n",
    "            if(count > 50):\n",
    "                save_text(text, output_dir, input_filename)\n",
    "                return 1\n",
    "            \n",
    "            else:      \n",
    "\n",
    "                return 0\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "# preprocess pdf text and  save in directory\n",
    "def preprocess_one_doc_from_pdf(input_dir, input_filename, output_dir):\n",
    "    ps = PorterStemmer()\n",
    "    input_file_path = input_dir + \"\\\\\"+ input_filename\n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    ret_val = 999\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'rb') as content_file:\n",
    "            for line in content_file:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if(line in ['\\n', '\\r\\n','\\r']):\n",
    "                    continue\n",
    "                    \n",
    "                line = line.strip()\n",
    "                line = remove_hyper_link(line)\n",
    "                line = remove_special_char(line)\n",
    "                line = line.lower()\n",
    "                line = re.sub(' +',' ',line)\n",
    "                words = line.split(\" \")\n",
    "\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    word = remove_special_char(word)\n",
    "                    word = re.sub(' +','',word)\n",
    "                    \n",
    "                    if word not in stopwords and word != \" \" and word != \"\":\n",
    "                        stem_word = ps.stem(word)\n",
    "                        text = text + \" \" + stem_word\n",
    "                        count = count + 1\n",
    "                        \n",
    "            if(count > 50):\n",
    "                save_text(text, output_dir, input_filename)\n",
    "                return 1\n",
    "            \n",
    "            else:\n",
    "                return  0\n",
    "    except:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# crawl through single webpage\n",
    "def webpage_crawler(total_number_docs):\n",
    "    global doc_count \n",
    "    global link_queue\n",
    "    global last_doc_index\n",
    "    global start_time\n",
    "    global crawled_web_dir\n",
    "    global crawled_web_dir_conv_need\n",
    "    \n",
    "    if(doc_count % 100 == 0 and last_doc_index != doc_count):\n",
    "        print(\"Extracted Documents: \" + str(doc_count))\n",
    "        last_doc_index = doc_count\n",
    "        \n",
    "    if(doc_count % 200 == 0):\n",
    "        format_time(start_time, time.time())\n",
    "        \n",
    "    url = link_queue.get()\n",
    "    #print(doc_count+1, \" : \", url)\n",
    "    \n",
    "    try:\n",
    "        link_extention = get_page_extention(url)\n",
    "        \n",
    "        if(url == \"\" or link_extention == \"ppt\"):\n",
    "            a=1\n",
    "        elif(link_extention in [\"pdf\",\"docx\", \"pptx\"]):\n",
    "            import_convert_preprocess(url, link_extention) \n",
    "        elif(link_extention == \"txt\"):\n",
    "            fetch_extract_html_txt(url)\n",
    "        else:\n",
    "            fetch_extract_html_txt(url)\n",
    "            \n",
    "    except: \n",
    "        pass        \n",
    "\n",
    "\n",
    "    \n",
    "# crawl through a website            \n",
    "def website_crawler(total_number_docs):\n",
    "    global link_queue\n",
    "    global doc_count\n",
    "    global total_number_doc\n",
    "    \n",
    "    while(doc_count < total_number_docs):\n",
    "        if(link_queue.empty()):\n",
    "            print(\"Queue is empty\")\n",
    "            return\n",
    "        \n",
    "        if(doc_count%200 == 0 and doc_count != 0):\n",
    "            save_all_obj()\n",
    "            \n",
    "        webpage_crawler(total_number_docs)\n",
    "        \n",
    "    save_all_obj()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#web crawler main\n",
    "def web_crawling_main(url, domain, total_page_count):\n",
    "    print(\"Start Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
    "    \n",
    "    delete_all_files()\n",
    "    reset_global_variables()\n",
    "    \n",
    "    create_directories(list_dir)\n",
    "    \n",
    "    load_stopwords(stopword_path)\n",
    "    \n",
    "    url = remove_slash_before_or_after(url, \"after\")\n",
    "    link_original = strip_http_s(url)\n",
    "    page_queued_map[link_original] = 1\n",
    "    print(url)\n",
    "    link_queue.put(url)\n",
    "    \n",
    "    total_number_docs = total_page_count\n",
    "    \n",
    "    website_crawler(total_number_docs)\n",
    "    \n",
    "\n",
    "# Update crawling from last saved url location    \n",
    "def web_crawling_main_update(url, domain, num_add_doc):\n",
    "    global page_doc_map\n",
    "    global doc_page_map\n",
    "    global page_ref_count\n",
    "    global total_number_docs\n",
    "    global doc_count\n",
    "    \n",
    "    print(\"Start Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
    "    \n",
    "    reset_global_variables()\n",
    "    load_stopwords(stopword_path)\n",
    "    load_all_obj()\n",
    "\n",
    "    total_number_docs = doc_count + num_add_doc\n",
    "    website_crawler(total_number_docs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; \n",
    "        y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#build inverted index for all files present in preprocessed file directory\n",
    "def inverse_document_indexer(preprocessed_file_dir_path):\n",
    "    dirs = os.listdir(preprocessed_file_dir_path)\n",
    "    i = 0 \n",
    "    \n",
    "    for file in dirs:\n",
    "        filepath = preprocessed_file_dir_path + \"\\\\\"+ file\n",
    "        text = \"\"\n",
    "        i = i + 1\n",
    "        \n",
    "        if(i % 1000 == 0):\n",
    "            print(\"Building inverse document index for file no: \"+str(i))   \n",
    "            print(\"Current Time: \", datetime.datetime.time(datetime.datetime.now()))\n",
    "            \n",
    "        try:\n",
    "            with open(filepath, 'r') as content_file:\n",
    "                file_name = str(file)[:-4]\n",
    "                \n",
    "                doc_term_freq_vector[file_name] = {}\n",
    "                single_doc_term_freq_vector = doc_term_freq_vector[file_name]\n",
    "                \n",
    "                for line in content_file:\n",
    "                    line = line.strip()\n",
    "                    words = line.split(\" \")\n",
    "                    \n",
    "                    for word in words:\n",
    "                        word = word.strip()\n",
    "                        \n",
    "                        if word != \"\":\n",
    "                            if word not in term_doc_freq_vector:\n",
    "                                single_term_doc_freq_vector = {}\n",
    "                                single_term_doc_freq_vector[file_name] = 1\n",
    "                                single_term_doc_freq_vector[\"DocFreq\"] = 1  \n",
    "                                term_doc_freq_vector[word] = single_term_doc_freq_vector\n",
    "\n",
    "                            else:\n",
    "\n",
    "                                single_term_doc_freq_vector = term_doc_freq_vector[word]\n",
    "\n",
    "                                if file_name not in single_term_doc_freq_vector:\n",
    "                                    single_term_doc_freq_vector[file_name] = 1\n",
    "                                    single_term_doc_freq_vector[\"DocFreq\"] = single_term_doc_freq_vector[\"DocFreq\"] + 1\n",
    "                                    term_doc_freq_vector[word] = single_term_doc_freq_vector\n",
    "\n",
    "                                else:\n",
    "                                    single_term_doc_freq_vector[file_name] = single_term_doc_freq_vector[file_name] + 1\n",
    "                                    term_doc_freq_vector[word] = single_term_doc_freq_vector \n",
    "\n",
    "                            a=1\n",
    "                            if \"DocMaxFreq\" not in single_doc_term_freq_vector:\n",
    "                                single_doc_term_freq_vector[\"DocMaxFreq\"] = 1\n",
    "\n",
    "                            if word not in single_doc_term_freq_vector:\n",
    "                                single_doc_term_freq_vector[word] = 1\n",
    "                                doc_term_freq_vector[file_name] = single_doc_term_freq_vector\n",
    "                                \n",
    "                            else:\n",
    "                                single_doc_term_freq_vector[word] = single_doc_term_freq_vector[word] + 1\n",
    "                                \n",
    "                                if(single_doc_term_freq_vector[word] > single_doc_term_freq_vector[\"DocMaxFreq\"]):\n",
    "                                    single_doc_term_freq_vector[\"DocMaxFreq\"] = single_doc_term_freq_vector[word]\n",
    "                                doc_term_freq_vector[file_name] = single_doc_term_freq_vector\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "           \n",
    "            \n",
    "#builds tfidf from inverse document index\n",
    "def tfidf_document_text(term_doc_freq_vector, doc_term_freq_vector):\n",
    "    global total_number_docs\n",
    "    \n",
    "    total_number_docs = load_obj(\"doc_count.p\")\n",
    "    \n",
    "    doc_term_freq_vector_normalized = doc_term_freq_vector\n",
    "\n",
    "    \n",
    "    for doc in doc_term_freq_vector_normalized:\n",
    "        for term in doc_term_freq_vector_normalized[doc]:\n",
    "            \n",
    "            if(term != \"DocMaxFreq\"):\n",
    "                doc_freq = term_doc_freq_vector[term][\"DocFreq\"]\n",
    "                doc_term_freq_vector_normalized[doc][term] = (doc_term_freq_vector[doc][term]/doc_term_freq_vector[doc][\"DocMaxFreq\"])*(math.log2(total_number_docs/doc_freq))\n",
    "    \n",
    "    \n",
    "    for doc in doc_term_freq_vector_normalized:\n",
    "        del doc_term_freq_vector_normalized[doc][\"DocMaxFreq\"]\n",
    "        \n",
    "    return doc_term_freq_vector_normalized\n",
    "\n",
    "\n",
    "\n",
    "def inverse_document_indexer_final(crawled_web_dir_preprocessed, stopword_path):\n",
    "    inverse_document_indexer(crawled_web_dir_preprocessed)\n",
    "    doc_term_freq_vector_norm = tfidf_document_text(term_doc_freq_vector, doc_term_freq_vector)\n",
    "    save_all_obj_tfidf(doc_term_freq_vector_norm)\n",
    "    \n",
    "    return term_doc_freq_vector, doc_term_freq_vector, doc_term_freq_vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query preprocessing \n",
    "def query_preprocessor(query_str):\n",
    "    ps = PorterStemmer()\n",
    "    query_dict = {}\n",
    "    \n",
    "    query_str_modified = query_str.strip()\n",
    "    query_str_modified = remove_special_char(query_str_modified)\n",
    "    query_str_modified = query_str_modified.lower()\n",
    "    query_str_modified = re.sub(' +',' ',query_str_modified)\n",
    "    words = query_str_modified.split(\" \")\n",
    "    \n",
    "    max_freq = 0\n",
    "    N = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        \n",
    "        if word not in stopwords and word !=\"\": \n",
    "            word = ps.stem(word)\n",
    "            \n",
    "            if word not in query_dict:\n",
    "                query_dict[word] = 1\n",
    "            else:\n",
    "                query_dict[word] = query_dict[word] + 1\n",
    "                \n",
    "            if(query_dict[word] > max_freq):\n",
    "                max_freq = query_dict[word]\n",
    "                \n",
    "            N +=1\n",
    "            \n",
    "    return query_dict, max_freq, N\n",
    "\n",
    "\n",
    "#generate normalized term vector for query\n",
    "def query_normalizer(query_dict, max_freq, total_number_docs, term_doc_freq_vector):\n",
    "    query_dict_normalized = {}\n",
    "    doc_term_freq_vector_normalized = doc_term_freq_vector \n",
    "    \n",
    "    for word in query_dict:\n",
    "        if word in term_doc_freq_vector:\n",
    "            query_dict_normalized[word] =  ( 0.5  +  (0.5 * query_dict[word] / max_freq) ) * (math.log2((total_number_docs+1)/(term_doc_freq_vector[word][\"DocFreq\"]+1)))\n",
    "        else:\n",
    "            query_dict_normalized[word] =  ( 0.5  +  (0.5 * query_dict[word] / max_freq) ) * (math.log2((total_number_docs+1)))\n",
    "    \n",
    "    return query_dict_normalized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# retrieve documents for query\n",
    "def retrieve_docs_with_query_word(query_term_freq_vect_norm, term_doc_freq_vector):\n",
    "    docs_with_query_terms = []\n",
    "    \n",
    "    for word in query_term_freq_vect_norm:\n",
    "        if word in term_doc_freq_vector:\n",
    "            docs = term_doc_freq_vector[word]\n",
    "            for doc in docs:\n",
    "                if(doc != \"DocFreq\"):\n",
    "                    docs_with_query_terms.append(doc)\n",
    "                    \n",
    "    return docs_with_query_terms\n",
    "\n",
    "# retrieve relevant document\n",
    "def calculate_cosine_query_doc(docs_with_query_terms, query_term_freq_norm, term_doc_freq_vector, doc_term_freq_vector_norm, doc_term_freq_vector):\n",
    "    cosine_query_doc = {}\n",
    "    \n",
    "    for doc in docs_with_query_terms:\n",
    "        temp = {}\n",
    "        \n",
    "        for word in doc_term_freq_vector_norm[doc]:\n",
    "            if word in query_term_freq_norm:\n",
    "                temp[word] = query_term_freq_norm[word]\n",
    "            else:\n",
    "                temp[word] = 0\n",
    "                \n",
    "        doc_v=[]\n",
    "        query_v = []\n",
    "        \n",
    "        for word in doc_term_freq_vector_norm[doc]:\n",
    "            doc_v.append(doc_term_freq_vector_norm[doc][word])\n",
    "            query_v.append(temp[word])\n",
    "        cosine_query_doc[doc] = cosine_similarity(doc_v, query_v)\n",
    "    \n",
    "    return cosine_query_doc\n",
    "\n",
    "\n",
    "#get url from file namedoc\n",
    "def get_url(cosine_query_doc, doc_url_map):\n",
    "    url_list = []\n",
    "    similarity = []\n",
    "    doc_list = []\n",
    "    similarity_map = {}\n",
    "    \n",
    "    cosine_query_doc_new = sorted(cosine_query_doc.items(), key=operator.itemgetter(1), reverse = True)  \n",
    "    cosine_query_doc_newest = {}\n",
    "    \n",
    "    for doc in cosine_query_doc_new:\n",
    "        cosine_query_doc_newest[doc[0]] = doc[1]\n",
    "        \n",
    "    for doc in cosine_query_doc_newest:\n",
    "        url_list.append(doc_url_map[int(doc)])\n",
    "        similarity.append(cosine_query_doc_newest[doc])\n",
    "        similarity_map[doc] = cosine_query_doc_newest[doc]\n",
    "        doc_list.append(doc)\n",
    "        \n",
    "    return url_list, doc_url_map, similarity, similarity_map, doc_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "#finds relevant docs\n",
    "def relevant_doc(query_term_freq_vect, doc_term_freq_vector):\n",
    "    relevant_list = []\n",
    "    relevant_list_map = {}\n",
    "    \n",
    "    for doc in doc_term_freq_vector:\n",
    "        doc_i = 0\n",
    "        \n",
    "        for term in query_term_freq_vect:\n",
    "            if term not in doc_term_freq_vector[doc]:\n",
    "                doc_i = -1\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                doc_i = doc_i + 1\n",
    "                \n",
    "        if(doc_i!=-1):\n",
    "            relevant_list.append(doc)\n",
    "            relevant_list_map[doc] = doc_i\n",
    "            \n",
    "            \n",
    "    return len(relevant_list), relevant_list, relevant_list_map\n",
    "\n",
    "\n",
    "#calculate number of relevant docs\n",
    "def num_relevant_doc_in_query(doc_list, query_term_freq_vect, doc_term_freq_vector):\n",
    "    relevant_list = []\n",
    "    relevant_list_map = {}\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        doc_i = 0\n",
    "        \n",
    "        for term in query_term_freq_vect:\n",
    "            if term not in doc_term_freq_vector[str(doc)]:\n",
    "                doc_i = -1\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                doc_i = doc_i + 1\n",
    "                \n",
    "        if(doc_i!=-1):\n",
    "            relevant_list.append(doc)\n",
    "            relevant_list_map[doc] = doc_i\n",
    "            \n",
    "    return len(relevant_list), relevant_list, relevant_list_map\n",
    "\n",
    "\n",
    "# find precision, recall and f1 score\n",
    "def evaluation(num, relevant_list_len, qrelevant_list_len):\n",
    "    print(num, relevant_list_len, qrelevant_list_len)\n",
    "    recall = -999\n",
    "    precision = -999\n",
    "    f1 = -999\n",
    "    \n",
    "    if relevant_list_len != 0:\n",
    "        recall = qrelevant_list_len/relevant_list_len\n",
    "    \n",
    "    if num != 0:\n",
    "        precision = qrelevant_list_len/num\n",
    "        \n",
    "    if recall != -999 or precision != -999:\n",
    "        if recall == -999:\n",
    "            recall = 0\n",
    "        elif precision == -999 :\n",
    "            precision = 0\n",
    "            \n",
    "        f1= (2*precision*recall)/(precision + recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def perfomance(query_str, num):\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_f1 = 0\n",
    "    query_str_len = len(query_str)\n",
    "    i=1\n",
    "    \n",
    "    for query in query_str:\n",
    "        url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect,doc_term_freq_vector,doc_list = web_search_main(query_str[query])\n",
    "        relevant_list_len, relevant_list, relevant_list_map = relevant_doc(query_term_freq_vect, doc_term_freq_vector)\n",
    "        qrelevant_list_len,relevant_list, qrelevant_list_map = num_relevant_doc_in_query(doc_list[:num], query_term_freq_vect, doc_term_freq_vector)\n",
    "        \n",
    "        precision, recall, f1 = evaluation(num, relevant_list_len, qrelevant_list_len)\n",
    "        \n",
    "        avg_precision = avg_precision + precision\n",
    "        avg_recall = avg_recall + recall\n",
    "        avg_f1 = avg_f1 + f1\n",
    "        \n",
    "        print(i,\" : \", query_str[query], \" : precision : \", precision, \", recall : \", recall, \", f1 : \", f1)\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "        \n",
    "    avg_precision = (avg_precision/query_str_len)\n",
    "    avg_recall = (avg_recall/query_str_len)\n",
    "    avg_f1 = (avg_f1/query_str_len)\n",
    "    print(\"Average precision : \", avg_precision)\n",
    "    print(\"Average recall : \", avg_recall)\n",
    "    print(\"Average f1 : \", avg_f1) \n",
    "    \n",
    "    \n",
    "    return avg_precision, avg_recall, avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main search function \n",
    "def web_search_main(query_str):\n",
    "    #time_1 = time.time()\n",
    "    global total_number_docs\n",
    "    global doc_url_map\n",
    "    global term_doc_freq_vector\n",
    "    global doc_term_freq_vector\n",
    "    \n",
    "    load_obj_search()\n",
    "    \n",
    "    query_term_freq_vect, max_freq, N = query_preprocessor(query_str)\n",
    "    query_term_freq_vect_norm = query_normalizer(query_term_freq_vect, max_freq, total_number_docs, term_doc_freq_vector)\n",
    "    docs_with_query_terms = retrieve_docs_with_query_word(query_term_freq_vect_norm, term_doc_freq_vector)\n",
    "    cosine_query_doc = calculate_cosine_query_doc(docs_with_query_terms, query_term_freq_vect_norm, term_doc_freq_vector, doc_term_freq_vector_norm, doc_term_freq_vector)\n",
    "    url_list, doc_url_map, similarity, similarity_map, doc_list = get_url(cosine_query_doc, doc_url_map)\n",
    "    #format_time(time_1, time.time())\n",
    "    \n",
    "    return url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect, doc_term_freq_vector,doc_list\n",
    "\n",
    "\n",
    "def search_engine_final_main(query_str, count):\n",
    "    i = 0\n",
    "    result = []\n",
    "    url_list, doc_url_map, similarity, similarity_map, docs_with_query_terms, term_doc_freq_vector, query_term_freq_vect, doc_term_freq_vector,doc_list = web_search_main(query_str) \n",
    "    \n",
    "    for url in url_list:\n",
    "        url_row = []\n",
    "        print(i+1, \". \", url, \"\\nSimillarity: \", similarity[i])\n",
    "        url_row.append(i + 1)\n",
    "        url_row.append(url)\n",
    "        url_row.append(similarity[i])\n",
    "        result.append(url_row)\n",
    "        i += 1\n",
    "        \n",
    "        if i>count:\n",
    "            break\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time:  10:53:51.468673\n",
      "web_text_crawled\n",
      "web_docs_crawled\n",
      "web_text_crawled\n",
      "web_docs_crawled\n",
      "web_text_preprocessed\n",
      "http://www.memphis.edu\n",
      "Extracted Documents: 0\n",
      "HH:Min:Sec > 0 hr 0.0 min 0.0sec\n",
      "Extracted Documents: 100\n",
      "saved objects\n",
      "Extracted Documents: 200\n",
      "HH:Min:Sec > 0 hr 5.483333333333333 min 0.0sec\n",
      "failed deleting: web_text_crawled\\219.txt\n",
      "Extracted Documents: 300\n",
      "saved objects\n",
      "Extracted Documents: 400\n",
      "HH:Min:Sec > 0 hr 8.333333333333334 min -5.684341886080802e-14sec\n",
      "Extracted Documents: 500\n",
      "failed deleting: web_text_crawled\\570.txt\n",
      "saved objects\n",
      "Extracted Documents: 600\n",
      "HH:Min:Sec > 0 hr 13.566666666666666 min 0.0sec\n",
      "failed deleting: web_text_crawled\\635.txt\n",
      "failed deleting: web_text_crawled\\673.txt\n",
      "Extracted Documents: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 800\n",
      "HH:Min:Sec > 0 hr 21.55 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 0 hr 21.566666666666666 min 0.0sec\n",
      "failed deleting: web_text_crawled\\871.txt\n",
      "failed deleting: web_text_crawled\\872.txt\n",
      "Extracted Documents: 900\n",
      "saved objects\n",
      "Extracted Documents: 1000\n",
      "HH:Min:Sec > 0 hr 26.733333333333334 min 0.0sec\n",
      "failed deleting: web_text_crawled\\1100.txt\n",
      "Extracted Documents: 1100\n",
      "saved objects\n",
      "Extracted Documents: 1200\n",
      "HH:Min:Sec > 0 hr 31.55 min 0.0sec\n",
      "Extracted Documents: 1300\n",
      "failed deleting: web_text_crawled\\1301.txt\n",
      "saved objects\n",
      "Extracted Documents: 1400\n",
      "HH:Min:Sec > 0 hr 37.78333333333333 min 0.0sec\n",
      "Extracted Documents: 1500\n",
      "saved objects\n",
      "Extracted Documents: 1600\n",
      "HH:Min:Sec > 0 hr 41.733333333333334 min 0.0sec\n",
      "failed deleting: web_text_crawled\\1656.txt\n",
      "failed deleting: web_text_crawled\\1656.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 1700\n",
      "failed deleting: web_text_crawled\\1756.txt\n",
      "failed deleting: web_text_crawled\\1756.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\1784.txt\n",
      "saved objects\n",
      "Extracted Documents: 1800\n",
      "HH:Min:Sec > 0 hr 59.86666666666667 min 0.0sec\n",
      "failed deleting: web_text_crawled\\1822.txt\n",
      "failed deleting: web_text_crawled\\1838.txt\n",
      "failed deleting: web_text_crawled\\1859.txt\n",
      "failed deleting: web_text_crawled\\1859.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1860.txt\n",
      "failed deleting: web_text_crawled\\1862.txt\n",
      "failed deleting: web_text_crawled\\1862.txt\n",
      "failed deleting: web_text_crawled\\1862.txt\n",
      "failed deleting: web_text_crawled\\1866.txt\n",
      "failed deleting: web_text_crawled\\1883.txt\n",
      "Extracted Documents: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\1966.txt\n",
      "failed deleting: web_text_crawled\\1979.txt\n",
      "failed deleting: web_text_crawled\\1984.txt\n",
      "failed deleting: web_text_crawled\\1993.txt\n",
      "saved objects\n",
      "Extracted Documents: 2000\n",
      "HH:Min:Sec > 1 hr 17.0 min 0.0sec\n",
      "Extracted Documents: 2100\n",
      "saved objects\n",
      "Extracted Documents: 2200\n",
      "HH:Min:Sec > 1 hr 21.766666666666666 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 1 hr 21.783333333333335 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\2261.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 2300\n",
      "failed deleting: web_text_crawled\\2327.txt\n",
      "saved objects\n",
      "Extracted Documents: 2400\n",
      "HH:Min:Sec > 1 hr 31.5 min 0.0sec\n",
      "failed deleting: web_text_crawled\\2451.txt\n",
      "Extracted Documents: 2500\n",
      "saved objects\n",
      "Extracted Documents: 2600\n",
      "HH:Min:Sec > 1 hr 35.28333333333333 min 0.0sec\n",
      "Extracted Documents: 2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 2800\n",
      "HH:Min:Sec > 1 hr 40.483333333333334 min 0.0sec\n",
      "Extracted Documents: 2900\n",
      "failed deleting: web_text_crawled\\2929.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 3000\n",
      "HH:Min:Sec > 1 hr 46.55 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 3100\n",
      "failed deleting: web_text_crawled\\3101.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\3145.txt\n",
      "failed deleting: web_text_crawled\\3145.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n",
      "failed deleting: web_text_crawled\\3146.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 3200\n",
      "HH:Min:Sec > 1 hr 51.2 min 0.0sec\n",
      "Extracted Documents: 3300\n",
      "saved objects\n",
      "Extracted Documents: 3400\n",
      "HH:Min:Sec > 1 hr 59.766666666666666 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 1 hr 59.78333333333333 min 0.0sec\n",
      "failed deleting: web_text_crawled\\3449.txt\n",
      "Extracted Documents: 3500\n",
      "failed deleting: web_text_crawled\\3594.txt\n",
      "saved objects\n",
      "Extracted Documents: 3600\n",
      "HH:Min:Sec > 2 hr 5.833333333333333 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 2 hr 5.833333333333333 min 0.0sec\n",
      "Extracted Documents: 3700\n",
      "failed deleting: web_text_crawled\\3717.txt\n",
      "failed deleting: web_text_crawled\\3722.txt\n",
      "failed deleting: web_text_crawled\\3765.txt\n",
      "failed deleting: web_text_crawled\\3781.txt\n",
      "failed deleting: web_text_crawled\\3783.txt\n",
      "saved objects\n",
      "Extracted Documents: 3800\n",
      "HH:Min:Sec > 2 hr 13.533333333333333 min 0.0sec\n",
      "failed deleting: web_text_crawled\\3830.txt\n",
      "failed deleting: web_text_crawled\\3841.txt\n",
      "failed deleting: web_text_crawled\\3845.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 3900\n",
      "failed deleting: web_text_crawled\\3914.txt\n",
      "failed deleting: web_text_crawled\\3968.txt\n",
      "failed deleting: web_text_crawled\\3981.txt\n",
      "failed deleting: web_text_crawled\\3981.txt\n",
      "saved objects\n",
      "Extracted Documents: 4000\n",
      "HH:Min:Sec > 2 hr 19.633333333333333 min 0.0sec\n",
      "Extracted Documents: 4100\n",
      "failed deleting: web_text_crawled\\4174.txt\n",
      "failed deleting: web_text_crawled\\4174.txt\n",
      "failed deleting: web_text_crawled\\4176.txt\n",
      "failed deleting: web_text_crawled\\4176.txt\n",
      "failed deleting: web_text_crawled\\4193.txt\n",
      "saved objects\n",
      "Extracted Documents: 4200\n",
      "HH:Min:Sec > 2 hr 23.8 min 0.0sec\n",
      "failed deleting: web_text_crawled\\4287.txt\n",
      "Extracted Documents: 4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\4382.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 4400\n",
      "HH:Min:Sec > 2 hr 37.3 min 0.0sec\n",
      "failed deleting: web_text_crawled\\4423.txt\n",
      "failed deleting: web_text_crawled\\4427.txt\n",
      "failed deleting: web_text_crawled\\4468.txt\n",
      "Extracted Documents: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 4600\n",
      "HH:Min:Sec > 2 hr 44.88333333333333 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 2 hr 44.95 min 0.0sec\n",
      "Extracted Documents: 4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\4720.txt\n",
      "saved objects\n",
      "Extracted Documents: 4800\n",
      "HH:Min:Sec > 2 hr 48.86666666666667 min 0.0sec\n",
      "Extracted Documents: 4900\n",
      "saved objects\n",
      "Extracted Documents: 5000\n",
      "HH:Min:Sec > 2 hr 57.0 min 0.0sec\n",
      "Extracted Documents: 5100\n",
      "saved objects\n",
      "Extracted Documents: 5200\n",
      "HH:Min:Sec > 3 hr 0.18333333333333332 min 0.0sec\n",
      "failed deleting: web_text_crawled\\5244.txt\n",
      "failed deleting: web_text_crawled\\5259.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\5312.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 5400\n",
      "HH:Min:Sec > 3 hr 48.68333333333333 min 0.0sec\n",
      "Extracted Documents: 5500\n",
      "saved objects\n",
      "Extracted Documents: 5600\n",
      "HH:Min:Sec > 3 hr 53.86666666666667 min 0.0sec\n",
      "Extracted Documents: 5700\n",
      "saved objects\n",
      "Extracted Documents: 5800\n",
      "HH:Min:Sec > 3 hr 58.416666666666664 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\5991.txt\n",
      "failed deleting: web_text_crawled\\5993.txt\n",
      "saved objects\n",
      "Extracted Documents: 6000\n",
      "HH:Min:Sec > 4 hr 5.516666666666667 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\6072.txt\n",
      "failed deleting: web_text_crawled\\6072.txt\n",
      "failed deleting: web_text_crawled\\6072.txt\n",
      "failed deleting: web_text_crawled\\6072.txt\n",
      "failed deleting: web_text_crawled\\6072.txt\n",
      "Extracted Documents: 6100\n",
      "saved objects\n",
      "Extracted Documents: 6200\n",
      "HH:Min:Sec > 4 hr 13.1 min 0.0sec\n",
      "failed deleting: web_text_crawled\\6248.txt\n",
      "failed deleting: web_text_crawled\\6295.txt\n",
      "failed deleting: web_text_crawled\\6295.txt\n",
      "failed deleting: web_text_crawled\\6295.txt\n",
      "failed deleting: web_text_crawled\\6295.txt\n",
      "failed deleting: web_text_crawled\\6295.txt\n",
      "Extracted Documents: 6300\n",
      "failed deleting: web_text_crawled\\6330.txt\n",
      "failed deleting: web_text_crawled\\6358.txt\n",
      "failed deleting: web_text_crawled\\6397.txt\n",
      "failed deleting: web_text_crawled\\6400.txt\n",
      "saved objects\n",
      "Extracted Documents: 6400\n",
      "HH:Min:Sec > 4 hr 26.883333333333333 min 0.0sec\n",
      "failed deleting: web_text_crawled\\6443.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\6448.txt\n",
      "failed deleting: web_text_crawled\\6449.txt\n",
      "failed deleting: web_text_crawled\\6449.txt\n",
      "failed deleting: web_text_crawled\\6490.txt\n",
      "failed deleting: web_text_crawled\\6491.txt\n",
      "Extracted Documents: 6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 6600\n",
      "HH:Min:Sec > 4 hr 33.516666666666666 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 4 hr 33.53333333333333 min 0.0sec\n",
      "failed deleting: web_text_crawled\\6671.txt\n",
      "Extracted Documents: 6700\n",
      "saved objects\n",
      "Extracted Documents: 6800\n",
      "HH:Min:Sec > 4 hr 38.46666666666667 min 0.0sec\n",
      "Extracted Documents: 6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 7000\n",
      "HH:Min:Sec > 4 hr 42.5 min 0.0sec\n",
      "Extracted Documents: 7100\n",
      "saved objects\n",
      "Extracted Documents: 7200\n",
      "HH:Min:Sec > 4 hr 46.833333333333336 min 0.0sec\n",
      "failed deleting: web_text_crawled\\7216.txt\n",
      "failed deleting: web_text_crawled\\7216.txt\n",
      "failed deleting: web_text_crawled\\7216.txt\n",
      "Extracted Documents: 7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 7400\n",
      "HH:Min:Sec > 4 hr 51.06666666666667 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 4 hr 51.1 min 0.0sec\n",
      "failed deleting: web_text_crawled\\7412.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 7500\n",
      "saved objects\n",
      "Extracted Documents: 7600\n",
      "HH:Min:Sec > 4 hr 55.38333333333333 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\7660.txt\n",
      "Extracted Documents: 7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\7743.txt\n",
      "failed deleting: web_text_crawled\\7764.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved objects\n",
      "Extracted Documents: 7800\n",
      "HH:Min:Sec > 5 hr 3.65 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "failed deleting: web_text_crawled\\7991.txt\n",
      "saved objects\n",
      "Extracted Documents: 8000\n",
      "HH:Min:Sec > 5 hr 21.75 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 8100\n",
      "failed deleting: web_text_crawled\\8110.txt\n",
      "failed deleting: web_text_crawled\\8135.txt\n",
      "saved objects\n",
      "Extracted Documents: 8200\n",
      "HH:Min:Sec > 5 hr 46.95 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\8227.txt\n",
      "failed deleting: web_text_crawled\\8231.txt\n",
      "failed deleting: web_text_crawled\\8232.txt\n",
      "failed deleting: web_text_crawled\\8232.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n",
      "failed deleting: web_text_crawled\\8237.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\8382.txt\n",
      "failed deleting: web_text_crawled\\8382.txt\n",
      "failed deleting: web_text_crawled\\8382.txt\n",
      "saved objects\n",
      "Extracted Documents: 8400\n",
      "HH:Min:Sec > 5 hr 56.03333333333333 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed deleting: web_text_crawled\\8482.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 8500\n",
      "failed deleting: web_text_crawled\\8572.txt\n",
      "failed deleting: web_text_crawled\\8572.txt\n",
      "saved objects\n",
      "Extracted Documents: 8600\n",
      "HH:Min:Sec > 6 hr 5.716666666666667 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 6 hr 5.733333333333333 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 6 hr 5.75 min 0.0sec\n",
      "Extracted Documents: 8700\n",
      "saved objects\n",
      "Extracted Documents: 8800\n",
      "HH:Min:Sec > 6 hr 10.916666666666666 min 0.0sec\n",
      "saved objects\n",
      "HH:Min:Sec > 6 hr 10.933333333333334 min 0.0sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 8900\n",
      "failed deleting: web_text_crawled\\8960.txt\n",
      "failed deleting: web_text_crawled\\8960.txt\n",
      "saved objects\n"
     ]
    }
   ],
   "source": [
    "total_page_count = 9000\n",
    "\n",
    "#web_crawling_main(url, domain, total_page_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_add_doc = 200\n",
    "#web_crawling_main_update(url, domain, num_add_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverse document index for file no: 1000\n",
      "Current Time:  20:16:38.381692\n",
      "Building inverse document index for file no: 2000\n",
      "Current Time:  20:16:39.927722\n",
      "Building inverse document index for file no: 3000\n",
      "Current Time:  20:16:41.403030\n",
      "Building inverse document index for file no: 4000\n",
      "Current Time:  20:16:43.437892\n",
      "Building inverse document index for file no: 5000\n",
      "Current Time:  20:16:45.534070\n",
      "Building inverse document index for file no: 6000\n",
      "Current Time:  20:16:47.140271\n",
      "Building inverse document index for file no: 7000\n",
      "Current Time:  20:16:48.394226\n",
      "Building inverse document index for file no: 8000\n",
      "Current Time:  20:16:52.068943\n"
     ]
    }
   ],
   "source": [
    "crawled_web_dir_preprocessed = \"web_text_preprocessed\"\n",
    "stopword_path = \"english.stopwords.txt\"\n",
    "term_doc_freq_vector, doc_term_freq_vector, doc_term_freq_vector_norm = inverse_document_indexer_final(crawled_web_dir_preprocessed, stopword_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_str = {\"q1\":\"international office\", \"q2\":\"software engineering research\", \"q3\":\"Cookie\", \"q4\":\"president of the university\",\"q5\":\"computer science research awards\",\n",
    "\"q6\":\"semantic similarity\", \"q7\":\"tiger bike's current offer\",\"q8\": \"where is the library located?\", \"q9\":\"How to graduate with honors?\",\"q10\":\"scholarships in computer science\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1073 52\n",
      "1  :  international office  : precision :  0.52 , recall :  0.048462255358807084 , f1 :  0.08866155157715261\n",
      "100 194 13\n",
      "2  :  software engineering research  : precision :  0.13 , recall :  0.06701030927835051 , f1 :  0.08843537414965986\n",
      "100 22 22\n",
      "3  :  Cookie  : precision :  0.22 , recall :  1.0 , f1 :  0.36065573770491804\n",
      "100 1145 91\n",
      "4  :  president of the university  : precision :  0.91 , recall :  0.0794759825327511 , f1 :  0.1461847389558233\n",
      "100 197 1\n",
      "5  :  computer science research awards  : precision :  0.01 , recall :  0.005076142131979695 , f1 :  0.006734006734006734\n",
      "100 27 4\n",
      "6  :  semantic similarity  : precision :  0.04 , recall :  0.14814814814814814 , f1 :  0.06299212598425197\n",
      "100 4 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bba3a2ad9ff1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mperfomance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1f0ccce0be84>\u001b[0m in \u001b[0;36mperfomance\u001b[1;34m(query_str, num)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mqrelevant_list_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrelevant_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqrelevant_list_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_relevant_doc_in_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_term_freq_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_term_freq_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_list_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqrelevant_list_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mavg_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavg_precision\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1f0ccce0be84>\u001b[0m in \u001b[0;36mevaluation\u001b[1;34m(num, relevant_list_len, qrelevant_list_len)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "num=100\n",
    "perfomance(query_str, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  http://www.memphis.edu/accounting/xls/sar_request.xlsx \n",
      "Simillarity:  0.7899958221734292\n",
      "2 .  http://www.memphis.edu/fcbescholarships/index.php \n",
      "Simillarity:  0.7899958221734292\n",
      "3 .  http://www.memphis.edu/fcbescholarships/graduate/index.php \n",
      "Simillarity:  0.7790636520951871\n",
      "4 .  http://www.memphis.edu/fcbescholarships/graduate \n",
      "Simillarity:  0.7790636520951871\n",
      "5 .  http://www.memphis.edu/scholarships/index.php \n",
      "Simillarity:  0.7762368495053653\n",
      "6 .  http://www.memphis.edu/scholarships \n",
      "Simillarity:  0.7762368495053653\n",
      "7 .  http://www.memphis.edu/scholarships/scholarships/policy.php \n",
      "Simillarity:  0.7642906166325293\n",
      "8 .  http://www.memphis.edu/ccfa/scholarships/index.php \n",
      "Simillarity:  0.7632836685921269\n",
      "9 .  http://www.memphis.edu/scholarships/pdfs/uofmtransferscholarships20182019.pdf \n",
      "Simillarity:  0.6794507105450291\n",
      "10 .  http://www.memphis.edu/accounting/xls/supply_advance.xlsx \n",
      "Simillarity:  0.6741399013920634\n",
      "11 .  http://www.memphis.edu/cehhs/scholarships \n",
      "Simillarity:  0.6741399013920634\n",
      "12 .  http://www.memphis.edu/coe/scholarships \n",
      "Simillarity:  0.6741399013920634\n",
      "13 .  http://www.memphis.edu/law/admissions/scholarships.php \n",
      "Simillarity:  0.6725297413974128\n",
      "14 .  http://www.memphis.edu/cas/scholarships/index.php \n",
      "Simillarity:  0.6515909457076583\n",
      "15 .  http://www.memphis.edu/cas/scholarships \n",
      "Simillarity:  0.6515909457076583\n",
      "16 .  http://www.memphis.edu/fcbescholarships/undergraduate/index.php \n",
      "Simillarity:  0.6304645386866863\n",
      "17 .  http://www.memphis.edu/fcbescholarships/undergraduate \n",
      "Simillarity:  0.6304645386866863\n",
      "18 .  http://www.memphis.edu/scholarships/pdfs/tigerscholarmanagerhowtoguide.pdf \n",
      "Simillarity:  0.6252289973012699\n",
      "19 .  http://www.memphis.edu/art/resources/scholarship.php \n",
      "Simillarity:  0.6106587252785393\n",
      "20 .  http://www.memphis.edu/scholarships/pdfs/tigerscholarmanagerhowtoguide2.pdf \n",
      "Simillarity:  0.6084001983316757\n",
      "21 .  http://www.memphis.edu/cas/scholarships/college-scholarships.php \n",
      "Simillarity:  0.6029283550691733\n",
      "22 .  http://www.memphis.edu/scholarships/scholarshipcheck.php \n",
      "Simillarity:  0.5969628507023587\n",
      "23 .  http://www.memphis.edu/libraries/technology/computerlabs.php \n",
      "Simillarity:  0.5749981678684424\n",
      "24 .  http://www.memphis.edu/libraries/collectionmgmt/pdfs/comp_06.pdf \n",
      "Simillarity:  0.554133705511492\n",
      "25 .  http://www.memphis.edu/cas/scholarships/departmental-scholarships.php \n",
      "Simillarity:  0.539936106660451\n",
      "26 .  http://www.memphis.edu/admissions/high-school/act_gpa_schol.php \n",
      "Simillarity:  0.5290568499456679\n",
      "27 .  http://www.memphis.edu/scholarships/about.php \n",
      "Simillarity:  0.5253160743781058\n",
      "28 .  http://www.memphis.edu/scholarships/pdfs/uofmfreshmenscholarships20182019.pdf \n",
      "Simillarity:  0.5162772249537882\n",
      "29 .  http://www.memphis.edu/wll/scholarships/index.php \n",
      "Simillarity:  0.5107328497097995\n",
      "30 .  http://www.memphis.edu/law/documents/firstyearscholarshipsforthewebrev13.pdf \n",
      "Simillarity:  0.5098499493753351\n",
      "31 .  http://www.memphis.edu/cs/programs/index.php \n",
      "Simillarity:  0.5096985894855278\n",
      "32 .  http://www.memphis.edu/law/documents/scholarshiphandout4.11.2013.pdf \n",
      "Simillarity:  0.5022811662201077\n",
      "33 .  http://www.memphis.edu/honors/opportunities/scholarships.php \n",
      "Simillarity:  0.4951893892646663\n",
      "34 .  http://www.memphis.edu/opportunity/prospective_students/equal_change_for_education_opportunity_application.pdf \n",
      "Simillarity:  0.4894724597913944\n",
      "35 .  http://www.memphis.edu/scholarships/pdfs/2018_2019_comprehensive_scholarship_information.pdf \n",
      "Simillarity:  0.48678636809206566\n",
      "36 .  http://www.memphis.edu/music/future/endowed.php \n",
      "Simillarity:  0.4766163637874055\n",
      "37 .  http://www.memphis.edu/music/future/scholshp.php \n",
      "Simillarity:  0.4720442226815491\n",
      "38 .  http://www.memphis.edu/veterans/scholarships/rudd-scholarship.php \n",
      "Simillarity:  0.4678245174175619\n",
      "39 .  http://www.memphis.edu/cs/people/index.php \n",
      "Simillarity:  0.4646495335447851\n",
      "40 .  http://www.memphis.edu/umparents/involvement/pfabookscholarship.php \n",
      "Simillarity:  0.4641517405679929\n",
      "41 .  http://www.memphis.edu/wilson/scholarships/index.php \n",
      "Simillarity:  0.46104325758221815\n",
      "42 .  http://www.memphis.edu/wilson/students/financial_aid.php \n",
      "Simillarity:  0.4606671231827887\n",
      "43 .  http://www.memphis.edu/opportunity/prospective_students/20162017theopportunityscholarshipapplication.pdf \n",
      "Simillarity:  0.4540991588261285\n",
      "44 .  http://www.memphis.edu/cs/pdfs/2017coll_watson.pdf \n",
      "Simillarity:  0.45228882170813167\n",
      "45 .  http://www.memphis.edu/fcbescholarships/apply/index.php \n",
      "Simillarity:  0.4495278814126325\n",
      "46 .  http://www.memphis.edu/accountancy/admissions/financial_assistance.php \n",
      "Simillarity:  0.4375957434692901\n",
      "47 .  http://www.memphis.edu/acss/adults/adultschol.php \n",
      "Simillarity:  0.4263779931884613\n",
      "48 .  http://www.memphis.edu/scholarships/scholarships/srvhropts.php \n",
      "Simillarity:  0.42405738436524054\n",
      "49 .  http://www.provets.org/pdf/ScholarshipApplication.pdf \n",
      "Simillarity:  0.42051156350324265\n",
      "50 .  http://www.memphis.edu/scholarships/scholarships/hopetransfer.php \n",
      "Simillarity:  0.4132880432555554\n",
      "51 .  http://www.memphis.edu/veterans/scholarships/additional.php \n",
      "Simillarity:  0.41117311353085756\n",
      "52 .  http://www.memphis.edu/law/admissions/tuition-financial-aid.php \n",
      "Simillarity:  0.4069777377523642\n",
      "53 .  http://www.memphis.edu/biology/scholarships/index.php \n",
      "Simillarity:  0.4030392037975668\n",
      "54 .  http://www.memphis.edu/veterans/scholarships/army-rotc.php \n",
      "Simillarity:  0.4023563804680255\n",
      "55 .  http://www.memphis.edu/cs/financial_aid/peter_neathery.php \n",
      "Simillarity:  0.3990294510791193\n",
      "56 .  http://www.memphis.edu/abroad/funding/scholarships.php \n",
      "Simillarity:  0.3986713700582109\n",
      "57 .  http://www.memphis.edu/law/alumni/awards-scholarships.php \n",
      "Simillarity:  0.39121406780963763\n",
      "58 .  https://blogs.memphis.edu/libraries/2015/04/14/university-libraries-hosts-faculty-scholarship-week-april-13-17 \n",
      "Simillarity:  0.385779505986981\n",
      "59 .  http://www.memphis.edu/cs/pdfs/2017coll_lee.pdf \n",
      "Simillarity:  0.38531856129366104\n",
      "60 .  http://www.memphis.edu/umtech/solutions/software/softwarefacultystaff.php \n",
      "Simillarity:  0.38523674228580185\n",
      "61 .  http://www.memphis.edu/scholarships/pdfs/community_college_transfer_scholarship_application.pdf \n",
      "Simillarity:  0.3825260154389255\n",
      "62 .  http://www.memphis.edu/scholarships/hope/hopetransfer.php \n",
      "Simillarity:  0.3800916504154376\n",
      "63 .  http://www.memphis.edu/abroad/funding \n",
      "Simillarity:  0.37423957119947115\n",
      "64 .  http://www.memphis.edu/veterans/scholarships/dependents.php \n",
      "Simillarity:  0.37176564902254394\n",
      "65 .  http://www.memphis.edu/cas/advising/major-brochures/computer_science.pdf \n",
      "Simillarity:  0.37165741839745614\n",
      "66 .  http://www.memphis.edu/scholarships/servicehours/index.php \n",
      "Simillarity:  0.3690573341001071\n",
      "67 .  http://www.memphis.edu/msci/ugrad/scholarships.php \n",
      "Simillarity:  0.3638606369838471\n",
      "68 .  http://www.memphis.edu/armyrotc/getting_started/sholarship_information.php \n",
      "Simillarity:  0.36152950591770255\n",
      "69 .  http://www.memphis.edu/scholarships/pdfs/summerhopescholarshipinfo.pdf \n",
      "Simillarity:  0.3607625461011125\n",
      "70 .  http://www.memphis.edu/law/documents/returning2ndand3rdyearlawstudentsscholarshipsforwebrev18.pdf \n",
      "Simillarity:  0.3577701352943974\n",
      "71 .  https://blogs.memphis.edu/libraries/2015/04 \n",
      "Simillarity:  0.3569711536631725\n",
      "72 .  http://www.memphis.edu/umparents/resources/po_transfer.php \n",
      "Simillarity:  0.35233421977559554\n",
      "73 .  http://www.memphis.edu/uofmglobal/degrees/undergraduate/political-science.php \n",
      "Simillarity:  0.3422815194956933\n",
      "74 .  http://www.memphis.edu/nursing/students/lcon_bsn_scholarship.php \n",
      "Simillarity:  0.3383929212800284\n",
      "75 .  http://www.memphis.edu/polisci/graduate/index.php \n",
      "Simillarity:  0.33595584642663273\n",
      "76 .  http://www.memphis.edu/scholarships/pdfs/hopenontraditionalweb.pdf \n",
      "Simillarity:  0.33491237264455176\n",
      "77 .  http://www.memphis.edu/veterans/scholarships/combat-medic.php \n",
      "Simillarity:  0.33212337864160446\n",
      "78 .  http://www.memphis.edu/veterans/scholarships/pronet.php \n",
      "Simillarity:  0.3303882497432617\n",
      "79 .  http://www.memphis.edu/umtech/service_catalog/classroom/index.php \n",
      "Simillarity:  0.329368101550762\n",
      "80 .  http://www.memphis.edu/scholarships/scholarships/hopefresh.php \n",
      "Simillarity:  0.3271788123503557\n",
      "81 .  http://www.memphis.edu/earthsciences/programs/index.php \n",
      "Simillarity:  0.3261317249770811\n",
      "82 .  http://www.memphis.edu/cas/about/mission.php \n",
      "Simillarity:  0.3261273956995953\n",
      "83 .  http://www.memphis.edu/cas/future-students/index.php \n",
      "Simillarity:  0.3232695678180082\n",
      "84 .  http://www.memphis.edu/cognitive-science/index.php \n",
      "Simillarity:  0.3182794636991821\n",
      "85 .  http://www.memphis.edu/earthsciences/about/index.php \n",
      "Simillarity:  0.3164450539704026\n",
      "86 .  http://www.memphis.edu/umparents/resources/hb_financial.php \n",
      "Simillarity:  0.31582512221259784\n",
      "87 .  http://www.memphis.edu/veterans/scholarships/tn-national-guard.php \n",
      "Simillarity:  0.3101776376818083\n",
      "88 .  http://www.memphis.edu/earthsciences \n",
      "Simillarity:  0.30904284590887154\n",
      "89 .  http://des.memphis.edu \n",
      "Simillarity:  0.30904284590887154\n",
      "90 .  http://www.memphis.edu/cs/pdfs/2017coll_michael.pdf \n",
      "Simillarity:  0.3044445619488489\n",
      "91 .  http://www.memphis.edu/abroad/students/how_to_apply.php \n",
      "Simillarity:  0.3037454960277647\n",
      "92 .  http://www.memphis.edu/veterans/scholarships/marines.php \n",
      "Simillarity:  0.301838334500608\n",
      "93 .  http://www.memphis.edu/cs/contact/index.php \n",
      "Simillarity:  0.2998037549219713\n",
      "94 .  http://www.memphis.edu//scholarships/pdfs/scholarshipextensionform.pdf \n",
      "Simillarity:  0.29978479932995716\n",
      "95 .  http://www.memphis.edu/cs/current_students/index.php \n",
      "Simillarity:  0.298816063689116\n",
      "96 .  http://www.memphis.edu/feinstone/contact.php \n",
      "Simillarity:  0.29812343796469654\n",
      "97 .  http://www.memphis.edu/foundation/docs/new_fund_11142016.pdf \n",
      "Simillarity:  0.2959810301646364\n",
      "98 .  http://www.memphis.edu/cas/about/index.php \n",
      "Simillarity:  0.295765282663552\n",
      "99 .  http://www.memphis.edu/csd/programs/addtlcourseworkma.php \n",
      "Simillarity:  0.2948418642554476\n",
      "100 .  http://www.memphis.edu/nursing/students/lcon_rn_bsn_scholarship.php \n",
      "Simillarity:  0.2937117390094323\n",
      "101 .  http://www.memphis.edu/libraries/lis/guest_comp_access.php \n",
      "Simillarity:  0.2917721812790746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_str= \"scholarships in computer science\"\n",
    "count = 100\n",
    "a=search_engine_final_main(query_str, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take input\n",
    "def input_file_dir():\n",
    "    url = input(\"Enter URL: \")  \n",
    "    web_text_dir = input(\"Enter Directory Name For Saving Fetched Web Documents In Text Format: \")\n",
    "    web_other_doc_dir = input(\"Enter Directory Name For Saving Fetched PDF/Other Documents: \")\n",
    "    web_preprocessed_dir = input(\"Enter Directory Name For Saving Text Files After Preprocessing: \")\n",
    "    web_output_dir = input(\"Enter Output Directory Name: \")\n",
    "    stopword_path = input(\"Enter Path Of File Containg Stopwords: \")\n",
    "\n",
    "    print(\"I will fetch web documents from -->\"+ url +\"\\nDocuments Will be fetched and parsed and saved in -->\"+ web_text_dir + \"\\nPDF or Other Documents will be saved in --> \"\n",
    "      + web_other_doc_dir + \"\\nAfter preprocessing files will be saved in -->\" + web_preprocessed_dir + \"\\nStop words are in file -->\" + stopword_path)\n",
    "\n",
    "#delete if file is empty\n",
    "def delete_empty_file(path):\n",
    "    if(os.path.getsize(path) == 0):\n",
    "        try:\n",
    "            print(\"deleting: \"+ path)\n",
    "            os.remove(path)\n",
    "            print(\"deleted: \"+ path)\n",
    "            return 1\n",
    "        except WindowsError:\n",
    "            return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def print_queue(queue):\n",
    "    for elem in list(q.queue):\n",
    "        print(elem)\n",
    "\n",
    "        \n",
    "\n",
    "#print inverted index of all terms present\n",
    "def print_inv_index():\n",
    "    for key in sorted(term_doc_freq_vector):\n",
    "        print(key)\n",
    "        for key2 in sorted(term_doc_freq_vector[key]):\n",
    "            print(str(key2) + \" : \" +  str(term_doc_freq_vector[key][key2]))\n",
    "        print(\"----------------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
