{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n* Name        : Assignment6, pdf2text\\n* Course      : Inform Retrieval/Web Search\\n* Assignment  : Assignment 6\\n* Description : Collects 10, 000 documents from \"www.memphis.edu\" and preproces, then builds inverted index.\\n* Author      : Anjana Tiha\\n* Date        : 11.09.2017\\n* Comments    : Please use Anaconda editor\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "* Name        : Assignment6, pdf2text\n",
    "* Course      : Inform Retrieval/Web Search\n",
    "* Assignment  : Assignment 6\n",
    "* Description : Collects 10, 000 documents from \"www.memphis.edu\" and preproces, then builds inverted index.\n",
    "* Author      : Anjana Tiha\n",
    "* Date        : 11.09.2017\n",
    "* Comments    : Please use Anaconda editor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import modules\n",
    "\n",
    "import urllib\n",
    "from urllib.parse import urlsplit\n",
    "import os, errno\n",
    "import time\n",
    "import operator \n",
    "import collections\n",
    "import queue\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global Hashmaps, variables\n",
    "stopwords = {}\n",
    "inv_ind = {}\n",
    "page_doc_map = {}\n",
    "page_ref_count = {}\n",
    "doc_count = 0\n",
    "link_queue = queue.Queue()\n",
    "last_doc_index = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crawled_web_dir = \"web_text_crawled\"\n",
    "crawled_web_dir_conv_need = \"web_docs_crawled\"\n",
    "crawled_web_dir_preprocessed = \"web_text_preprocessed\"\n",
    "output_web_dir = \"output\"\n",
    "\n",
    "stopword_path = \"english.stopwords.txt\"\n",
    "\n",
    "list_dir = [crawled_web_dir, crawled_web_dir_conv_need, crawled_web_dir_preprocessed]\n",
    "\n",
    "url = \"http://www.memphis.edu/\"\n",
    "domain = \"memphis.edu\"\n",
    "#url = \"http://www.cs.memphis.edu/~vrus/teaching/ir-websearch/\"\n",
    "#url = \"http://www.cs.memphis.edu/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create/Delete/Load stop words/\n",
    "\n",
    "#take input\n",
    "def input_file_dir():\n",
    "    url = input(\"Enter URL: \")  \n",
    "    web_text_dir = input(\"Enter Directory Name For Saving Fetched Web Documents In Text Format: \")\n",
    "    web_other_doc_dir = input(\"Enter Directory Name For Saving Fetched PDF/Other Documents: \")\n",
    "    web_preprocessed_dir = input(\"Enter Directory Name For Saving Text Files After Preprocessing: \")\n",
    "    web_output_dir = input(\"Enter Output Directory Name: \")\n",
    "    stopword_path = input(\"Enter Path Of File Containg Stopwords: \")\n",
    "\n",
    "    print(\"I will fetch web documents from -->\"+url+\"\\nDocuments Will be fetched and parsed and saved in -->\"+ web_text_dir + \"\\nPDF or Other Documents will be saved in --> \"\n",
    "      + web_other_doc_dir + \"\\nAfter preprocessing files will be saved in -->\" + web_preprocessed_dir + \"\\nStop words are in file -->\" + stopword_path)\n",
    "    \n",
    "    \n",
    "# create one single directory\n",
    "def create_directory(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if(e.errno != errno.EEXIST):\n",
    "            raise\n",
    "    pass \n",
    "\n",
    "\n",
    "# create a list of directories\n",
    "def create_directories(list_dir):\n",
    "    for dir_i in list_dir:\n",
    "        print(dir_i)\n",
    "        create_directory(dir_i) \n",
    "        \n",
    "# delete one single directory\n",
    "def delete_directory(dir_name):\n",
    "    if(os.path.isdir(dir_name)):\n",
    "        try:\n",
    "            shutil.rmtree(dir_name)\n",
    "        except OSError as e:\n",
    "            if(e.errno != errno.EEXIST):\n",
    "                raise\n",
    "        pass \n",
    "\n",
    "# delete a list of directories\n",
    "def delete_directories(list_dir):\n",
    "    for dir_i in list_dir:\n",
    "        print(dir_i)\n",
    "        delete_directory(dir_i)\n",
    "\n",
    "        \n",
    "#delete if file is empty\n",
    "def delete_file(path):\n",
    "    try:\n",
    "        os.remove(path)\n",
    "        return 1\n",
    "    except OSError:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "#delete if file is empty\n",
    "def delete_empty_file(path):\n",
    "    if(os.path.getsize(path) == 0):\n",
    "        try:\n",
    "            print(\"deleting: \"+ path)\n",
    "            os.remove(path)\n",
    "            print(\"deleted: \"+ path)\n",
    "            return 1\n",
    "        except WindowsError:\n",
    "            return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "#save text in given path with given file name\n",
    "def saveText(text, dir_path, file_name):\n",
    "    text_file = open(dir_path+\"\\\\\"+file_name, \"w\")\n",
    "    text_file.write(text)\n",
    "    \n",
    "    \n",
    "#load stop words from given file path\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r') as content_file:\n",
    "                for line in content_file:\n",
    "                    line = line.strip()\n",
    "                    stopwords[line] = 1\n",
    "\n",
    "                    \n",
    "# save object in pickle\n",
    "def save_obj(obj, name, key_or_val, order):\n",
    "    if(key_or_val == \"key\" and order == \"auto\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0))\n",
    "    elif(key_or_val == \"key\" and order == \"reverse\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(0), reverse=True)\n",
    "    elif(key_or_val == \"value\" and order == \"auto\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1))\n",
    "    elif(key_or_val == \"value\" and order == \"reverse\"):\n",
    "        sorted_x = sorted(obj.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    pickle.dump( obj, open( name + \".p\", \"wb\" ) )\n",
    "\n",
    "\n",
    "#load object from pickle file\n",
    "def load_obj(name):\n",
    "    file = open(name,'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    return object_file\n",
    "  \n",
    "        \n",
    "#print elapsed time in hh:mm:ss format\n",
    "def format_time(start_time, end_time):\n",
    "    elsapsed_time = end_time - start_time\n",
    "    hr = int(elsapsed_time)//3600\n",
    "    min_ = (int(elsapsed_time) - (hr * 3600))/60\n",
    "    sec = int(elsapsed_time) - hr * 3600 - min_ * 60\n",
    "    print(\"HH:Min:Sec > \" + str(hr) +\" hr \" + str(min_) + \" min \"+ str(sec) + \"sec\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print hashmap sorted by key or value\n",
    "def print_hashmap(hashmap, type_s):\n",
    "    if(type_s == 'key'):\n",
    "        hashmap = OrderedDict(sorted(hashmap.items(), key=lambda t: t[0]))\n",
    "    else:\n",
    "        hashmap = OrderedDict(sorted(hashmap.items(), key=lambda t: t[1], reverse=True))\n",
    "    for i in hashmap:\n",
    "        print(i, \" : \", hashmap[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URLS and Text preprocessing functions\n",
    "\n",
    "# remove fragment identifier #\n",
    "def remove_url_frag_id(url):\n",
    "    url = url.split('#')[0]\n",
    "    return url\n",
    "\n",
    "\n",
    "# remove http or https from webpage urls\n",
    "def strip_http_s(url):    \n",
    "    url = url.replace(\"https://\",\"\")\n",
    "    url = url.replace(\"http://\",\"\")\n",
    "    return url\n",
    "\n",
    "\n",
    "# check if url is in domain name\n",
    "def check_if_in_domain(url, domain):\n",
    "    if(domain in url):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "#fix URL with same name \"/\"\n",
    "def fix_url(url):\n",
    "    webpage_url = url.rsplit('/', 1)[0]\n",
    "    return webpage_url  \n",
    "\n",
    "\n",
    "#check whether URL is valid\n",
    "def check_valid_URL(url):\n",
    "    url_reg = re.compile(\n",
    "        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "        r'localhost|' #localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?' # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    is_valid = url_reg.match(url)\n",
    "    return is_valid\n",
    "\n",
    "\n",
    "#get extention of link to check the link type(.txt, .pdf, or html) \n",
    "def get_page_extention(url):\n",
    "    weblink_extention = url.rsplit('.', 1)[-1]\n",
    "    return weblink_extention\n",
    "\n",
    "\n",
    "#remove hyperlink from web page text for preprocessing\n",
    "def remove_hyper_link(text):\n",
    "    URLless_string = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', text)\n",
    "    return URLless_string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#remove special character in single line\n",
    "def remove_special_char(line):\n",
    "    line = re.sub('[^a-zA-Z]+', ' ', line)\n",
    "    return line\n",
    "\n",
    "        \n",
    "# get all the url/links to other pages from current pages        \n",
    "def get_all_links(html):\n",
    "    links_list = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    for tag in links:\n",
    "        link = tag.get('href',None)\n",
    "        if link is not None:\n",
    "            links_list.append(link)\n",
    "    return links_list   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converts from pdf txt and html\n",
    "\n",
    "# convert pdf to text using \"pdf2text\"\n",
    "def pdf_to_text(input_pdf, output_dir, file_name):\n",
    "    os.system((\"pdftotext %s %s\") %( input_pdf, output_dir+\"//\"+file_name))\n",
    "\n",
    "    \n",
    "#import text from url/single web link and convert to text and save in directory\n",
    "def import_convert_preprocess_pdf(url, pdf_directory, output_dir):\n",
    "    global doc_count\n",
    "    global crawled_web_dir\n",
    "    print(\"Extracting PDF............................................................................\")\n",
    "    url_map_name = strip_http_s(url)\n",
    "    print(url_map_name)\n",
    "    if(url_map_name not in page_doc_map):\n",
    "        page_doc_map[url_map_name] = -1\n",
    "        page_ref_count[url_map_name] = 1\n",
    "        \n",
    "        try:\n",
    "            doc_count_temp = doc_count + 1\n",
    "            book_name = str(doc_count_temp) + \".pdf\"\n",
    "            book_path = pdf_directory + \"/\" + book_name\n",
    "            a = requests.get(url, stream=True)\n",
    "            \n",
    "            with open(book_path, 'wb') as book:\n",
    "                book.write( url + \"\\n\")\n",
    "                \n",
    "                for block in a.iter_content(512):\n",
    "                    if not block:\n",
    "                        break\n",
    "                    book.write(block)\n",
    "\n",
    "            file_name = str(doc_count_temp)+\".txt\"\n",
    "            pdf_to_text(book_path, output_dir, file_name)\n",
    "            \n",
    "            \n",
    "            file_path = crawled_web_dir +\"\\\\\" + file_name\n",
    "\n",
    "            #checks if number of token > 50\n",
    "            is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, file_name, crawled_web_dir_preprocessed)\n",
    "\n",
    "            if(is_valid_for_indexing != 1):\n",
    "                delete_file(file_path)\n",
    "                page_doc_map[url_map_name] = -2\n",
    "\n",
    "            else:\n",
    "                doc_count = doc_count + 1\n",
    "                page_doc_map[url_map_name] = doc_count\n",
    "                page_ref_count[url_map_name] = 1\n",
    "        \n",
    "        except IOError:\n",
    "            page_doc_map[url_map_name]= -1\n",
    "            return \"\"\n",
    "    else:\n",
    "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
    "        \n",
    "        \n",
    "#remove all html and scripting\n",
    "def clean_html(html_text):\n",
    "    global crawled_web_dir\n",
    "    soup = BeautifulSoup(html_text)\n",
    "\n",
    "    # removes scripts and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # get text\n",
    "    clean_text = soup.get_text()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# preprocess files in a folder to remove punctuations, digits, special characters, url/web links\n",
    "# removes stop words given in file\n",
    "# convert to origin word/ do stemming\n",
    "\n",
    "def preprocess_one_doc(input_dir, input_filename, output_dir):\n",
    "    ps = PorterStemmer()\n",
    "    input_file_path = input_dir + \"\\\\\"+ input_filename\n",
    "    text = \"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        with open(input_file_path, 'r') as content_file:\n",
    "            for line in content_file:\n",
    "                if(line in ['\\n', '\\r\\n','\\r']):\n",
    "                    continue\n",
    "                line = line.strip()\n",
    "                line = remove_hyper_link(line)\n",
    "                line = remove_special_char(line)\n",
    "                line = line.lower()\n",
    "                line = re.sub(' +',' ',line)\n",
    "                words = line.split(\" \")\n",
    "\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    word = remove_special_char(word)\n",
    "                    word = re.sub(' +','',word)\n",
    "                    \n",
    "                    if word not in stopwords and word != \" \" and word != \"\":\n",
    "                        stem_word = ps.stem(word)\n",
    "                        text = text + \" \" + stem_word\n",
    "                        count = count + 1\n",
    "            if(count > 50):\n",
    "                saveText(text, output_dir, input_filename)\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    except:\n",
    "        print(\"Error in : \" + filepath)\n",
    "        return 0\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "# import text from single web page\n",
    "def fetch_extract_html_txt(url):\n",
    "    global doc_count\n",
    "    global domain\n",
    "    \n",
    "    if(check_if_in_domain(url, domain) == 0):\n",
    "        return \"\"\n",
    "    url_map_name = strip_http_s(url)\n",
    "    \n",
    "    if(url_map_name in page_doc_map):\n",
    "        page_ref_count[url_map_name] = page_ref_count[url_map_name] + 1\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "        page_doc_map[url_map_name] = -1\n",
    "        page_ref_count[url_map_name] = 1\n",
    "        \n",
    "        try: \n",
    "            html = urllib.request.urlopen(url) \n",
    "            html_text = html.read()  \n",
    "            \n",
    "            if(html_text == \"\"):\n",
    "                return \"\"\n",
    "            \n",
    "            clean_text = clean_html(html_text)\n",
    "            doc_count = doc_count + 1\n",
    "            page_doc_map[url_map_name] = doc_count\n",
    "            clean_text = url + \"\\n\" + clean_text \n",
    "            saveText(clean_text, crawled_web_dir, str(doc_count)+\".txt\")\n",
    "            \n",
    "            path = crawled_web_dir +\"\\\\\" + str(doc_count)+\".txt\"\n",
    "\n",
    "            #checks if number of token >= 50\n",
    "            is_valid_for_indexing = preprocess_one_doc(crawled_web_dir, str(doc_count)+\".txt\", crawled_web_dir_preprocessed)\n",
    "\n",
    "            if(is_valid_for_indexing != 1):\n",
    "                delete_file(path)\n",
    "                page_doc_map[url_map_name] = -2\n",
    "                doc_count = doc_count - 1\n",
    "                \n",
    "            links = get_all_links(html_text)\n",
    "            return links\n",
    "        \n",
    "        except IOError:\n",
    "            page_doc_map[url_map_name]= -1\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# crawl through whole website\n",
    "def webpage_crawler(doc_size):\n",
    "    global doc_count \n",
    "    global link_queue\n",
    "    global last_doc_index\n",
    "    \n",
    "    if(doc_count % 10 == 0 and last_doc_index != doc_count):\n",
    "        print(\"Extracted Documents: \" + str(doc_count))\n",
    "        last_doc_index = doc_count\n",
    "\n",
    "    url = link_queue.get()\n",
    "    links = []\n",
    "    \n",
    "    try:\n",
    "        link_extention = get_page_extention(url)\n",
    "        \n",
    "        if(link_extention == \"\" or link_extention == \"ppt\"):\n",
    "            a=1\n",
    "\n",
    "        elif(link_extention == \"pdf\"):\n",
    "            link_queue.put(link_q)\n",
    "            import_convert_preprocess_pdf(url, crawled_web_dir_conv_need, crawled_web_dir)\n",
    "            \n",
    "        elif(link_extention == \"txt\"):\n",
    "            fetch_extract_html_txt(url)\n",
    "            \n",
    "        else:\n",
    "            url = remove_url_frag_id(url)\n",
    "            links = fetch_extract_html_txt(url)\n",
    "\n",
    "    except Exception:\n",
    "        pass  \n",
    "    \n",
    "    if(links):\n",
    "        for link in links:\n",
    "            try:\n",
    "                link_extention = get_page_extention(link)\n",
    "                if(link_extention == \"\" or link_extention == \"ppt\"):\n",
    "                    a=1\n",
    "\n",
    "                elif(link_extention == \"pdf\"):\n",
    "                    url_new = fix_url(url)\n",
    "                    pdf_url = url_new  + link\n",
    "                    link_queue.put(pdf_url)\n",
    "\n",
    "                elif(link_extention == \"txt\"):\n",
    "                    if(check_valid_URL(link)):\n",
    "                        link_queue.put(link)\n",
    "                        \n",
    "                    else:\n",
    "                        url_new = fix_url(url)\n",
    "                        modified_link = url_new + link  \n",
    "                        \n",
    "                        if(check_valid_URL(modified_link)):\n",
    "                            link_queue.put(modified_link)\n",
    "                else:\n",
    "                    is_valid = check_valid_URL(link)\n",
    "                    \n",
    "                    if(is_valid):\n",
    "                        modified_link = fix_url(url)\n",
    "                        link_queue.put(modified_link)\n",
    "                    else:\n",
    "                        #modified_link = fix_url(url)\n",
    "                        modified_link = modified_link + link \n",
    "                        \n",
    "                        if(check_valid_URL(modified_link)):\n",
    "                            link_queue.put(modified_link)        \n",
    "            except Exception:\n",
    "                pass  \n",
    "\n",
    "            \n",
    "def website_crawler(doc_size):\n",
    "    while(doc_count <= doc_size):\n",
    "        if(link_queue.empty()):\n",
    "            return\n",
    "        webpage_crawler(doc_size)\n",
    "\n",
    "    save_obj(page_doc_map, \"url_doc_map\", \"value\", \"auto\")\n",
    "    save_obj(page_ref_count, \"page_ref_count\", \"value\", \"reverse\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build inverted index for all files present in preprocessed file directory\n",
    "def build_inv_file(path):\n",
    "    dirs = os.listdir( path )\n",
    "    i = 0 \n",
    "    \n",
    "    for file in dirs:\n",
    "        filepath = path + \"\\\\\"+ file\n",
    "        text = \"\"\n",
    "        i = i + 1\n",
    "        \n",
    "        if(i % 50 == 0):\n",
    "            print(\"Building inverse document index for file no: \"+str(i))\n",
    "            \n",
    "        try:\n",
    "            with open(filepath, 'r') as content_file:\n",
    "                for line in content_file:\n",
    "                    line = line.strip()\n",
    "                    words = line.split(\" \")\n",
    "                    \n",
    "                    for word in words:\n",
    "                        if word not in inv_ind:\n",
    "                            doc_ind = {}\n",
    "                            doc_ind[str(file)[:-4]] = 1\n",
    "                            doc_ind[\"Total Count\"] = 1\n",
    "                            inv_ind[word] = doc_ind\n",
    "                            \n",
    "                        else:\n",
    "                            doc_ind = inv_ind[word]\n",
    "                            \n",
    "                            if file not in doc_ind:\n",
    "                                doc_ind[str(file)[:-4]] = 1\n",
    "                                doc_ind[\"Total Count\"] = doc_ind[\"Total Count\"] + 1\n",
    "                                inv_ind[word] = doc_ind\n",
    "                                \n",
    "                            else:\n",
    "                                doc_ind[str(file)[:-4]] = doc_ind[str(file)[:-4]] + 1\n",
    "                                doc_ind[\"Total Count\"] = doc_ind[\"Total Count\"] + 1\n",
    "                                inv_ind[word] = doc_ind \n",
    "        \n",
    "        except:\n",
    "            print(\"Error in : \" + filepath)\n",
    "                \n",
    "\n",
    "#print inverted index of all terms present\n",
    "def print_inv_index():\n",
    "    for key in sorted(inv_ind):\n",
    "        print(key)\n",
    "        for key2 in sorted(inv_ind[key]):\n",
    "            print(str(key2) + \" : \" +  str(inv_ind[key][key2]))\n",
    "        print(\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "web_text_crawled\n",
      "web_docs_crawled\n",
      "web_text_preprocessed\n",
      "web_text_crawled\n",
      "web_docs_crawled\n",
      "web_text_preprocessed\n",
      "Extracted Documents: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anjana\\Anaconda\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Anjana\\Anaconda\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Documents: 10\n",
      "Extracted Documents: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_stopwords(stopword_path)\n",
    "\n",
    "delete_directories(list_dir)\n",
    "\n",
    "create_directories(list_dir)\n",
    "\n",
    "link_queue.put(url)\n",
    "\n",
    "doc_size = 10000\n",
    "\n",
    "website_crawler(doc_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH:Min:Sec > 0 hr 2.3666666666666667 min 0.0sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "format_time(start_time, time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "build_inv_file(crawled_web_dir_preprocessed)\n",
    "#save_obj(inv_ind, \"inv_ind\", \"value\", \"reverse\")\n",
    "#print_inv_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
